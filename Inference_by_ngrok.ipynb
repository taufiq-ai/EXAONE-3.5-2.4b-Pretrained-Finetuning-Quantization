{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNDFS/b0FnGBpxlbSaZq64z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taufiq-ai/EXAONE-3.5-2.4b-Pretrained-Finetuning-Quantization/blob/main/Inference_by_ngrok.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Dependencies"
      ],
      "metadata": {
        "id": "gWHLgR1u5Zs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flask & Tunneling\n",
        "! pip install pyngrok --upgrade\n",
        "! pip install flask-ngrok --upgrade\n",
        "! pip install flask-cors"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EfPdxD9az307"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformers Model\n",
        "! pip install torch transformers huggingface_hub tiktoken structlog"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ggZHal4q4w7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "oizHGvqCLQyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tunnel\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "import torch\n",
        "from pyngrok import ngrok\n",
        "ngrok.kill()\n",
        "\n",
        "# Models\n",
        "import warnings\n",
        "import tiktoken\n",
        "import structlog\n",
        "from bs4 import BeautifulSoup\n",
        "from huggingface_hub import login\n",
        "from torch import bfloat16\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "\n",
        "# Setup API Keys\n",
        "from google.colab import userdata\n",
        "NGROK_TOKEN = userdata.get('NGROK_TOKEN')\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "login(HF_TOKEN)\n",
        "! ngrok config add-authtoken $NGROK_TOKEN\n",
        "\n",
        "# logging\n",
        "logger = structlog.get_logger(__name__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dZP1_wg5H58",
        "outputId": "deb37d18-9aef-4031-e542-5f9a7d5f6ec2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "-O0wrHmdN1_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_model(\n",
        "    model_name_or_local_path:str=\"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\",\n",
        "    device=\"auto\",\n",
        "):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_local_path)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name_or_local_path,\n",
        "        torch_dtype=bfloat16,\n",
        "        trust_remote_code=True,\n",
        "        device_map=device,\n",
        "    )\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def infer_model(prompt, tokenizer, model, max_tokens=200, device=\"cuda\"):\n",
        "    if type(prompt)==str:\n",
        "        messages = [\n",
        "            {\"role\": \"system\",\n",
        "             \"content\": \"You are a helpful e-commerce customer support chatbot.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    else: messages=prompt\n",
        "\n",
        "    logger.info(\"Inference Started\", messages=messages)\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    output = model.generate(\n",
        "        input_ids.to(device),\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        max_new_tokens=max_tokens,\n",
        "        do_sample=False,\n",
        "    )\n",
        "    completion = tokenizer.decode(output[0])\n",
        "    content = completion.split(\"[|assistant|]\")[-1].split(\"[|endofturn|]\")[0]\n",
        "    logger.info(\"Generation Done\", content=content)\n",
        "    return completion, content"
      ],
      "metadata": {
        "id": "CFxY0jKvN0JU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Model & Tunneling"
      ],
      "metadata": {
        "id": "mWAlVIoON6cY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = setup_model()"
      ],
      "metadata": {
        "id": "7pWVYlNeQ4Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_inferecne(payload:dict, device=\"cuda\"):\n",
        "    prompt = payload.get('prompt')\n",
        "    max_tokens = payload.get('max_tokens')\n",
        "    logger.info(\"Inference Starting\", prompt=prompt, max_tokens=max_tokens)\n",
        "    completion, content = infer_model(prompt, tokenizer, model, max_tokens=200, device=device)\n",
        "    return content\n",
        "\n",
        "def run_app():\n",
        "    app = Flask(__name__)\n",
        "    CORS(app)  # Add this line after creating Flask app\n",
        "\n",
        "    # Start ngrok\n",
        "    public_url = ngrok.connect(\n",
        "        addr=5000,  # Your Flask app port\n",
        "        domain=\"closely-vital-puma.ngrok-free.app\"  # Your static domain (if any), otherwise comment\n",
        "    )\n",
        "    print(f\" * Public URL: {public_url}\")\n",
        "\n",
        "    @app.route('/gpu-inference', methods=['POST'])\n",
        "    def flask_inference():\n",
        "        try:\n",
        "            payload = request.json\n",
        "            logger.info(\"Received inference request\", payload=payload)\n",
        "            result = handle_inferecne(payload=payload)\n",
        "\n",
        "            # Proper response formatting\n",
        "            return jsonify({\n",
        "                \"status\": \"success\",\n",
        "                \"result\": result\n",
        "            }), 200\n",
        "        except Exception as e:\n",
        "            return jsonify({\n",
        "                \"status\": \"error\",\n",
        "                \"message\": str(e)\n",
        "            }), 500\n",
        "\n",
        "    app.run(port=5000)"
      ],
      "metadata": {
        "id": "bTU79GGtO_bs"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_app()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tM2GZucRGvR",
        "outputId": "ff06ef89-b2e8-4ca5-f5bb-00b6a2d1c982"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Public URL: NgrokTunnel: \"https://closely-vital-puma.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-01-09 23:47:36 [info     ] Received inference request     payload={'prompt': 'Hi Exa! How are you?', 'max_tokens': 200}\n",
            "2025-01-09 23:47:36 [info     ] Inference Starting             max_tokens=200 prompt=Hi Exa! How are you?\n",
            "2025-01-09 23:47:36 [info     ] Inference Started              messages=[{'role': 'system', 'content': 'You are a helpful e-commerce customer support chatbot.'}, {'role': 'user', 'content': 'Hi Exa! How are you?'}]\n",
            "2025-01-09 23:47:41 [info     ] Generation Done                content=Hello! Thank you for asking. As an AI assistant, I don't experience feelings like humans do, but I'm here and ready to help you with any questions or concerns you might have about products, orders, or anything else related to our services! How can I assist you today?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [09/Jan/2025 23:47:41] \"POST /gpu-inference HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-01-09 23:49:02 [info     ] Received inference request     payload={'prompt': 'Hi Exa! Is LD is a japanese tobaco brand?', 'max_tokens': 200}\n",
            "2025-01-09 23:49:02 [info     ] Inference Starting             max_tokens=200 prompt=Hi Exa! Is LD is a japanese tobaco brand?\n",
            "2025-01-09 23:49:02 [info     ] Inference Started              messages=[{'role': 'system', 'content': 'You are a helpful e-commerce customer support chatbot.'}, {'role': 'user', 'content': 'Hi Exa! Is LD is a japanese tobaco brand?'}]\n",
            "2025-01-09 23:49:11 [info     ] Generation Done                content=Hello! You're asking about LD, which could refer to different products depending on the context, especially in the tobacco industry where brand names can vary widely across regions. If you're referring to a specific tobacco brand named \"LD,\" it's important to clarify which country or region you're interested in, as tobacco brands often have localized names or variations.\n",
            "\n",
            "For example:\n",
            "- In Japan, tobacco brands might have names that reflect local preferences or historical significance, but \"LD\" isn't a widely recognized name for a major Japanese tobacco brand.\n",
            "- Outside Japan, there could be smaller or niche brands with similar names, but without more specific details, it's challenging to pinpoint exactly which brand you're asking about.\n",
            "\n",
            "If you could provide more details such as the country or any additional context about the product (like packaging, flavor, etc.), I could offer more precise information!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [09/Jan/2025 23:49:11] \"POST /gpu-inference HTTP/1.1\" 200 -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How Inference works with public endpoint?\n"
      ],
      "metadata": {
        "id": "R8vfGjSwejZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "headers = {'Content-Type': 'application/json'}\n",
        "endpoint = \"https://closely-vital-puma.ngrok-free.app/\"+\"gpu-inference\"\n",
        "payload = {\"prompt\":\"Hi Exa! How are you?\", \"max_tokens\":200}\n",
        "response = requests.post( endpoint, json=payload, headers=headers, timeout=30)\n",
        "response.json()"
      ],
      "metadata": {
        "id": "8UlvwpfhUsLg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}